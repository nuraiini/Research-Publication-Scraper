{
  "cells": [
    {
      "source": [
        "!pip install selenium"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vWV0QUsKkx1E",
        "outputId": "095aebb9-1b2b-4919-b301-6c0041d7b8cd"
      },
      "id": "vWV0QUsKkx1E",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting selenium\n",
            "  Downloading selenium-4.33.0-py3-none-any.whl.metadata (7.5 kB)\n",
            "Requirement already satisfied: urllib3~=2.4.0 in /usr/local/lib/python3.11/dist-packages (from urllib3[socks]~=2.4.0->selenium) (2.4.0)\n",
            "Collecting trio~=0.30.0 (from selenium)\n",
            "  Downloading trio-0.30.0-py3-none-any.whl.metadata (8.5 kB)\n",
            "Collecting trio-websocket~=0.12.2 (from selenium)\n",
            "  Downloading trio_websocket-0.12.2-py3-none-any.whl.metadata (5.1 kB)\n",
            "Requirement already satisfied: certifi>=2025.4.26 in /usr/local/lib/python3.11/dist-packages (from selenium) (2025.6.15)\n",
            "Collecting typing_extensions~=4.13.2 (from selenium)\n",
            "  Downloading typing_extensions-4.13.2-py3-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: websocket-client~=1.8.0 in /usr/local/lib/python3.11/dist-packages (from selenium) (1.8.0)\n",
            "Requirement already satisfied: attrs>=23.2.0 in /usr/local/lib/python3.11/dist-packages (from trio~=0.30.0->selenium) (25.3.0)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.11/dist-packages (from trio~=0.30.0->selenium) (2.4.0)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from trio~=0.30.0->selenium) (3.10)\n",
            "Collecting outcome (from trio~=0.30.0->selenium)\n",
            "  Downloading outcome-1.3.0.post0-py2.py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: sniffio>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from trio~=0.30.0->selenium) (1.3.1)\n",
            "Collecting wsproto>=0.14 (from trio-websocket~=0.12.2->selenium)\n",
            "  Downloading wsproto-1.2.0-py3-none-any.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from urllib3[socks]~=2.4.0->selenium) (1.7.1)\n",
            "Requirement already satisfied: h11<1,>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from wsproto>=0.14->trio-websocket~=0.12.2->selenium) (0.16.0)\n",
            "Downloading selenium-4.33.0-py3-none-any.whl (9.4 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m9.4/9.4 MB\u001b[0m \u001b[31m61.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trio-0.30.0-py3-none-any.whl (499 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m499.2/499.2 kB\u001b[0m \u001b[31m31.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trio_websocket-0.12.2-py3-none-any.whl (21 kB)\n",
            "Downloading typing_extensions-4.13.2-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m45.8/45.8 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n",
            "Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: wsproto, typing_extensions, outcome, trio, trio-websocket, selenium\n",
            "  Attempting uninstall: typing_extensions\n",
            "    Found existing installation: typing_extensions 4.14.0\n",
            "    Uninstalling typing_extensions-4.14.0:\n",
            "      Successfully uninstalled typing_extensions-4.14.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\n",
            "typeguard 4.4.3 requires typing_extensions>=4.14.0, but you have typing-extensions 4.13.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed outcome-1.3.0.post0 selenium-4.33.0 trio-0.30.0 trio-websocket-0.12.2 typing_extensions-4.13.2 wsproto-1.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Scrapping Data website scopus\n",
        "\n",
        "Scraping data dari website Scopus dilakukan dengan tujuan untuk mengumpulkan informasi publikasi ilmiah yang telah diterbitkan oleh seorang penulis atau institusi tertentu. Informasi ini meliputi judul artikel, jenis dokumen, daftar penulis, nama jurnal, serta tahun dan volume publikasi."
      ],
      "metadata": {
        "id": "pZNj7FsG1SQR"
      },
      "id": "pZNj7FsG1SQR"
    },
    {
      "cell_type": "code",
      "source": [
        "from selenium import webdriver\n",
        "from time import sleep\n",
        "from selenium.webdriver.support.ui import Select\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "# Install Firefox and geckodriver\n",
        "# Use !apt-get to install system packages in Colab\n",
        "#!apt-get update\n",
        "#!apt-get install firefox\n",
        "#!apt-get install geckodriver\n",
        "\n",
        "# Add geckodriver to PATH (might be needed if not automatically added)\n",
        "import os"
      ],
      "metadata": {
        "id": "VFUZazMXtduu"
      },
      "id": "VFUZazMXtduu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import re\n",
        "import csv\n",
        "import json\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.firefox.options import Options as FirefoxOptions\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support.ui import WebDriverWait, Select\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "from selenium.common.exceptions import TimeoutException, NoSuchElementException\n",
        "from selenium.webdriver.common.keys import Keys\n",
        "import random\n",
        "import pandas as pd\n",
        "\n",
        "# os.environ[\"PATH\"] += os.pathsep + \"/usr/bin\" # Ini mungkin tidak lagi diperlukan di Colab, tapi tidak ada salahnya jika ada.\n",
        "\n",
        "url = \"https://id.elsevier.com/as/authorization.oauth2?platSite=SC%2Fscopus&ui_locales=en-US&scope=openid+profile+email+els_auth_info+els_analytics_info+urn%3Acom%3Aelsevier%3Aidp%3Apolicy%3Aproduct%3Aindv_identity&els_policy=idp_policy_indv_identity_plus&response_type=code&redirect_uri=https%3A%2F%2Fwww.scopus.com%2Fauthredirect.uri%3FtxGid%3Dac07d4502027ce2ccc2422f962384f29&state=userLogin%7CtxId%3DD692FE65AD98B626728847C86FFE0CA7.i-068cc22061b9f8798%3A3&authType=SINGLE_SIGN_IN&prompt=login&client_id=SCOPUS\"\n",
        "\n",
        "options = FirefoxOptions()\n",
        "options.add_argument(\"--headless\")\n",
        "options.add_argument(\"--no-sandbox\")\n",
        "options.add_argument(\"--disable-dev-shm-usage\")\n",
        "options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 Safari/537.36\")\n",
        "\n",
        "def main_scopus_scraper():\n",
        "    driver = None\n",
        "    try:\n",
        "        driver = webdriver.Firefox(options=options)\n",
        "        print(\"üöÄ Memulai Scopus Scraper...\")\n",
        "        driver.get(url)\n",
        "        time.sleep(random.uniform(3, 7))\n",
        "\n",
        "        # --- Klik tombol \"Accept Cookies\" ---\n",
        "        try:\n",
        "            accept_cookies = WebDriverWait(driver, 20).until(\n",
        "                EC.element_to_be_clickable((By.ID, \"onetrust-accept-btn-handler\"))\n",
        "            )\n",
        "            accept_cookies.click()\n",
        "            print(\"‚úÖ Tombol 'Accept Cookies' diklik.\")\n",
        "            time.sleep(random.uniform(1, 3))\n",
        "        except TimeoutException:\n",
        "            print(\"‚ùó Tombol 'Accept Cookies' tidak muncul atau tidak dapat diklik dalam waktu yang ditentukan. Melanjutkan tanpa klik.\")\n",
        "            pass\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error saat mencoba klik tombol cookies: {e}\")\n",
        "            driver.save_screenshot(\"cookie_error.png\")\n",
        "\n",
        "\n",
        "        # --- Masukkan Email ---\n",
        "        input_email = WebDriverWait(driver, 20).until(\n",
        "            EC.element_to_be_clickable((By.ID, \"bdd-email\"))\n",
        "        )\n",
        "        input_email.clear()\n",
        "        input_email.send_keys(\"nur.aiiniinuraini@apps.ipb.ac.id\")\n",
        "        print(\"‚úÖ Email dimasukkan.\")\n",
        "        time.sleep(random.uniform(1, 3))\n",
        "\n",
        "        # --- Klik tombol \"Continue\" setelah email ---\n",
        "        continue_after_email = WebDriverWait(driver, 20).until(\n",
        "            EC.element_to_be_clickable((By.XPATH, \"//*[@id='bdd-elsPrimaryBtn']\"))\n",
        "        )\n",
        "        continue_after_email.click()\n",
        "        print(\"‚úÖ Tombol 'Continue' setelah email diklik.\")\n",
        "        time.sleep(random.uniform(5, 10))\n",
        "\n",
        "\n",
        "        # --- Cek halaman setelah email (kemungkinan OTP/verifikasi) ---\n",
        "        current_url_after_email = driver.current_url.lower()\n",
        "        page_source_after_email = driver.page_source.lower()\n",
        "\n",
        "        if \"login\" in current_url_after_email and \"password\" not in page_source_after_email:\n",
        "            print(\"‚ùó Kemungkinan terjadi pengalihan atau memerlukan verifikasi tambahan setelah email. Tidak ada kolom password.\")\n",
        "            driver.save_screenshot(\"login_email_issue.png\")\n",
        "            return\n",
        "\n",
        "\n",
        "        # --- Masukkan Password ---\n",
        "        input_pass = WebDriverWait(driver, 20).until(\n",
        "            EC.element_to_be_clickable((By.ID, \"bdd-password\"))\n",
        "        )\n",
        "        input_pass.clear()\n",
        "        input_pass.send_keys(\"Nuraini101#\")\n",
        "        print(\"‚úÖ Password dimasukkan.\")\n",
        "        time.sleep(random.uniform(1, 3))\n",
        "\n",
        "        # --- Klik tombol \"Sign In\" ---\n",
        "        sign_up = WebDriverWait(driver, 20).until(\n",
        "            EC.element_to_be_clickable((By.XPATH, \"//*[@id='bdd-elsPrimaryBtn']\"))\n",
        "        )\n",
        "        sign_up.click()\n",
        "        print(\"‚úÖ Tombol 'Sign In' diklik.\")\n",
        "        time.sleep(random.uniform(7, 15))\n",
        "\n",
        "\n",
        "        # --- PENTING: Pengecekan setelah login (URL dan konten) ---\n",
        "        current_url_after_login = driver.current_url.lower()\n",
        "        page_source_after_login = driver.page_source.lower()\n",
        "\n",
        "        if \"scopus.com\" not in current_url_after_login:\n",
        "            print(f\"‚ùó Gagal login atau tidak dialihkan ke domain Scopus. Current URL: {driver.current_url}\")\n",
        "            driver.save_screenshot(\"post_login_issue_domain.png\")\n",
        "            return\n",
        "        elif \"authentication failed\" in page_source_after_login or \\\n",
        "             \"invalid username or password\" in page_source_after_login or \\\n",
        "             \"login was unsuccessful\" in page_source_after_login:\n",
        "            print(\"‚ùó Login gagal: Kredensial tidak valid atau masalah autentikasi.\")\n",
        "            driver.save_screenshot(\"login_failed_credentials.png\")\n",
        "            return\n",
        "        elif \"captcha\" in page_source_after_login or \"unusual traffic\" in page_source_after_login:\n",
        "            print(\"‚ùó Terdeteksi CAPTCHA atau blokir setelah login.\")\n",
        "            driver.save_screenshot(\"captcha_post_login.png\")\n",
        "            return\n",
        "        else:\n",
        "            print(f\"‚úÖ Berhasil login atau dialihkan ke Scopus. Current URL: {driver.current_url}\")\n",
        "            try:\n",
        "                WebDriverWait(driver, 15).until(\n",
        "                    EC.presence_of_element_located((By.ID, \"author\"))\n",
        "                )\n",
        "                print(\"‚úÖ Elemen kunci ('Author' button) setelah login berhasil ditemukan.\")\n",
        "            except TimeoutException:\n",
        "                print(\"‚ùó Timeout: Elemen kunci setelah login tidak ditemukan. Halaman mungkin belum selesai memuat atau struktur berubah.\")\n",
        "                driver.save_screenshot(\"post_login_element_timeout.png\")\n",
        "                return\n",
        "\n",
        "\n",
        "        # --- Klik tombol \"Authors\" ---\n",
        "        authors = WebDriverWait(driver, 20).until(\n",
        "            EC.element_to_be_clickable((By.XPATH, \"//*[@id='author']\"))\n",
        "        )\n",
        "        authors.click()\n",
        "        print(\"‚úÖ Tombol 'Author' diklik.\")\n",
        "        time.sleep(random.uniform(2, 4))\n",
        "\n",
        "\n",
        "        # --- Masukkan Nama Belakang dan Nama Depan ---\n",
        "        Last_name = WebDriverWait(driver, 20).until(\n",
        "            EC.element_to_be_clickable((By.NAME, \"st1\"))\n",
        "        )\n",
        "        Last_name.clear()\n",
        "        Last_name.send_keys(\"Susetyo\")\n",
        "        print(\"‚úÖ Nama belakang dimasukkan: Susetyo\")\n",
        "        time.sleep(random.uniform(1, 2))\n",
        "\n",
        "        first_name = WebDriverWait(driver, 20).until(\n",
        "            EC.element_to_be_clickable((By.NAME, \"st2\"))\n",
        "        )\n",
        "        first_name.clear()\n",
        "        first_name.send_keys(\"Budi\")\n",
        "        print(\"‚úÖ Nama depan dimasukkan: Budi\")\n",
        "        time.sleep(random.uniform(3, 6))\n",
        "\n",
        "\n",
        "        # --- Klik tombol \"Search\" ---\n",
        "        try:\n",
        "            # Menggunakan XPath yang Anda berikan\n",
        "            search_button_locator = (By.XPATH, '//*[@id=\"scopus-author-search-form-experimental\"]/div[3]/div[2]/button/span[1]')\n",
        "\n",
        "            # Pastikan elemen ada di DOM\n",
        "            search_button_element = WebDriverWait(driver, 30).until(\n",
        "                EC.presence_of_element_located(search_button_locator)\n",
        "            )\n",
        "\n",
        "            # --- Tambahan: Tunggu hingga tombol tidak lagi disabled ---\n",
        "            WebDriverWait(driver, 10).until_not(\n",
        "                EC.element_attribute_to_include(search_button_locator, \"disabled\")\n",
        "            )\n",
        "            # Dan juga tunggu hingga elemen yang diwakili oleh locator menjadi clickable\n",
        "            WebDriverWait(driver, 10).until(\n",
        "                EC.element_to_be_clickable(search_button_locator)\n",
        "            )\n",
        "\n",
        "            time.sleep(random.uniform(1, 3)) # Beri sedikit jeda setelah tombol siap\n",
        "            search_button_element.click() # Klik elemen yang sudah ditemukan\n",
        "            print(\"‚úÖ Tombol 'Search' diklik.\")\n",
        "            time.sleep(random.uniform(5, 10)) # Penundaan setelah klik pencarian\n",
        "        except TimeoutException:\n",
        "            print(\"‚ùó Timeout: Tombol 'Search' tidak ditemukan atau tidak menjadi clickable dalam waktu yang ditentukan.\")\n",
        "            driver.save_screenshot(\"search_button_timeout.png\")\n",
        "            return\n",
        "        except NoSuchElementException:\n",
        "            print(\"‚ùó NoSuchElementException: Tombol 'Search' tidak ditemukan di DOM.\")\n",
        "            driver.save_screenshot(\"search_button_no_such_element.png\")\n",
        "            return\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error umum saat mengklik tombol Search: {e}\")\n",
        "            driver.save_screenshot(\"search_button_error.png\")\n",
        "            return\n",
        "\n",
        "\n",
        "        # --- Pengecekan halaman hasil pencarian penulis ---\n",
        "        if \"author.uri\" not in driver.current_url.lower() and \"search\" not in driver.current_url.lower():\n",
        "            print(\"‚ùó Gagal mencapai halaman hasil pencarian penulis. Current URL:\", driver.current_url)\n",
        "            driver.save_screenshot(\"author_search_results_issue.png\")\n",
        "            return\n",
        "        else:\n",
        "            print(\"‚úÖ Berhasil mencapai halaman hasil pencarian penulis.\")\n",
        "\n",
        "        # --- Klik profil penulis ---\n",
        "        search_name = WebDriverWait(driver, 20).until(\n",
        "            EC.element_to_be_clickable((\n",
        "                By.XPATH,\n",
        "                \"//a[contains(@title, \\\"View this author's profile\\\")]\"\n",
        "            ))\n",
        "        )\n",
        "        search_name.click()\n",
        "        print(\"‚úÖ Profil penulis diklik.\")\n",
        "        time.sleep(random.uniform(5, 10))\n",
        "\n",
        "\n",
        "        # --- Scroll perlahan ke bawah agar data fully loaded (optional) ---\n",
        "        print(\"‚è≥ Menggulir halaman profil penulis...\")\n",
        "        for i in range(0, 3000, 500):\n",
        "            driver.execute_script(f\"window.scrollTo(0, {i});\")\n",
        "            time.sleep(random.uniform(0.5, 1.5))\n",
        "        print(\"‚úÖ Selesai menggulir halaman.\")\n",
        "\n",
        "\n",
        "        wait = WebDriverWait(driver, 30)\n",
        "\n",
        "\n",
        "        # --- Tangani dropdown \"Display\" ---\n",
        "        try:\n",
        "            dropdown_container = wait.until(EC.presence_of_element_located((By.XPATH, \"//label[.//span[text()='Display']]\")))\n",
        "            select_element = dropdown_container.find_element(By.CSS_SELECTOR, \"select\")\n",
        "            select = Select(select_element)\n",
        "\n",
        "            available_options = [o.get_attribute('value') for o in select.options]\n",
        "            print(f\"Opsi display yang tersedia: {available_options}\")\n",
        "\n",
        "            if '200' in available_options:\n",
        "                select.select_by_value('200')\n",
        "                print(\"‚úÖ Memilih '200' item per halaman.\")\n",
        "                time.sleep(random.uniform(5, 10))\n",
        "                for i in range(0, 3000, 500):\n",
        "                    driver.execute_script(f\"window.scrollTo(0, {i});\")\n",
        "                    time.sleep(random.uniform(0.5, 1.5))\n",
        "            else:\n",
        "                print(\"Opsi '200' tidak tersedia. Menggunakan pengaturan default.\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error saat mengatur opsi display: {e}\")\n",
        "            driver.save_screenshot(\"display_option_error.png\")\n",
        "\n",
        "\n",
        "        # --- Mengambil data publikasi ---\n",
        "        print(\"‚è≥ Mengambil data publikasi...\")\n",
        "        containers = wait.until(EC.presence_of_all_elements_located(\n",
        "            (By.CSS_SELECTOR, 'li[data-testid=\"results-list-item\"]')\n",
        "        ))\n",
        "\n",
        "        print(f\"Jumlah data ditemukan: {len(containers)}\")\n",
        "\n",
        "        data = []\n",
        "\n",
        "        for container in containers:\n",
        "            article_text = ''\n",
        "            title_text = ''\n",
        "            authors_text = ''\n",
        "            journal_text = ''\n",
        "            year_detail_text = ''\n",
        "            article_url = '' # <<< Variabel untuk URL artikel\n",
        "\n",
        "            # Article Type\n",
        "            try:\n",
        "                article_text = container.find_element(By.CSS_SELECTOR, 'div[data-testid=\"document-type-container\"]').text.strip()\n",
        "            except NoSuchElementException:\n",
        "                pass\n",
        "\n",
        "            # Title and Article URL\n",
        "            try:\n",
        "                # Cari link <a> yang membungkus judul\n",
        "                title_link_element = container.find_element(By.CSS_SELECTOR, 'div.elements-module__IeohX h4 a')\n",
        "                title_text = title_link_element.text.strip()\n",
        "                article_url = title_link_element.get_attribute('href') # <<< Ambil atribut href\n",
        "            except NoSuchElementException:\n",
        "                # Jika link tidak ditemukan, coba ambil teks dari h4 saja (tanpa link)\n",
        "                try:\n",
        "                    title_element = container.find_element(By.CSS_SELECTOR, 'div.elements-module__IeohX h4')\n",
        "                    title_text = title_element.text.strip()\n",
        "                except NoSuchElementException:\n",
        "                    pass\n",
        "                article_url = '' # Jika tidak ada link, URL kosong\n",
        "\n",
        "\n",
        "            # Authors\n",
        "            try:\n",
        "                author_elements = container.find_elements(By.CSS_SELECTOR, 'div[data-testid=\"author-list\"] span.Authors-module__umR1O span')\n",
        "                authors_texts = [a.text.strip() for a in author_elements if a.text.strip()]\n",
        "                authors_text = ', '.join(authors_texts)\n",
        "            except NoSuchElementException:\n",
        "                pass\n",
        "\n",
        "            # Journal & Year/Details (Perbaikan di sini)\n",
        "            try:\n",
        "                # Scopus sering memiliki struktur di mana detail jurnal/konferensi\n",
        "                # dan tahun/volume ada di elemen Typography-module__...\n",
        "                # Mari kita coba ambil semua teks dari elemen yang relevan di bagian detail\n",
        "                # dan kemudian ekstrak tahun dari sana.\n",
        "\n",
        "                # Mengambil semua elemen yang mungkin berisi detail jurnal/tahun\n",
        "                detail_elements = container.find_elements(By.CSS_SELECTOR, 'span.Typography-module__lVnit.Typography-module__fRnrd.Typography-module__Nfgvc')\n",
        "\n",
        "                journal_parts = []\n",
        "                year_part = ''\n",
        "                volume_part = ''\n",
        "\n",
        "                for element in detail_elements:\n",
        "                    text = element.text.strip()\n",
        "                    if text:\n",
        "                        # Coba cari tahun (4 digit)\n",
        "                        year_match = re.search(r'\\b(19|20)\\d{2}\\b', text) # Mencari tahun antara 1900-2099\n",
        "                        if year_match:\n",
        "                            year_part = year_match.group(0)\n",
        "                            # Hapus tahun dari teks untuk menyisakan jurnal/konferensi\n",
        "                            text = text.replace(year_part, '').strip()\n",
        "\n",
        "                        # Coba cari volume/issue (seringkali dalam format angka atau V(I))\n",
        "                        volume_match = re.search(r'\\b(Vol\\.?\\s*\\d+|Issue\\s*\\d+|V\\d+\\s*\\(\\d+\\))\\b', text, re.IGNORECASE)\n",
        "                        if volume_match:\n",
        "                            volume_part = volume_match.group(0)\n",
        "                            text = text.replace(volume_part, '').strip()\n",
        "\n",
        "                        # Sisa teks dianggap bagian dari jurnal/konferensi\n",
        "                        if text:\n",
        "                            journal_parts.append(text.strip('., ')) # Hapus koma/titik di akhir\n",
        "\n",
        "                journal_text = ', '.join(filter(None, journal_parts)) # Gabungkan bagian jurnal yang ditemukan\n",
        "                year_detail_text = ''\n",
        "                if year_part:\n",
        "                    year_detail_text += year_part\n",
        "                if volume_part:\n",
        "                    if year_detail_text:\n",
        "                        year_detail_text += ', '\n",
        "                    year_detail_text += volume_part\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"‚ö† Peringatan: Gagal mengurai jurnal/tahun/volume untuk publikasi: {e}\")\n",
        "                journal_text = ''\n",
        "                year_detail_text = ''\n",
        "\n",
        "\n",
        "            data.append({\n",
        "                'Article': article_text,\n",
        "                'Title': title_text,\n",
        "                'Authors': authors_text,\n",
        "                'Journal': journal_text,\n",
        "                'Year/Volume': year_detail_text,\n",
        "                'Article URL': article_url # <<< Tambahkan URL ke data\n",
        "            })\n",
        "\n",
        "        df = pd.DataFrame(data)\n",
        "        print(\"‚úÖ Scraping Selesai. DataFrame dibuat.\")\n",
        "        print(df.head())\n",
        "        df.to_csv(\"scopus_data.csv\", index=False, encoding='utf-8')\n",
        "        print(\"‚úÖ Data disimpan ke 'scopus_data.csv'\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Terjadi kesalahan umum selama scraping: {e}\")\n",
        "        if driver:\n",
        "            driver.save_screenshot(\"general_error.png\")\n",
        "            print(\"Screenshot 'general_error.png' disimpan.\")\n",
        "    finally:\n",
        "        if driver:\n",
        "            driver.quit()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main_scopus_scraper()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jM1qen4g0YNK",
        "outputId": "8372486b-c480-4d3f-e21a-748d936eb9ef"
      },
      "id": "jM1qen4g0YNK",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üöÄ Memulai Scopus Scraper...\n",
            "‚úÖ Tombol 'Accept Cookies' diklik.\n",
            "‚úÖ Email dimasukkan.\n",
            "‚úÖ Tombol 'Continue' setelah email diklik.\n",
            "‚úÖ Password dimasukkan.\n",
            "‚úÖ Tombol 'Sign In' diklik.\n",
            "‚úÖ Berhasil login atau dialihkan ke Scopus. Current URL: https://www.scopus.com/pages/home?display=basic#basic\n",
            "‚úÖ Elemen kunci ('Author' button) setelah login berhasil ditemukan.\n",
            "‚úÖ Tombol 'Author' diklik.\n",
            "‚úÖ Nama belakang dimasukkan: Susetyo\n",
            "‚úÖ Nama depan dimasukkan: Budi\n",
            "‚úÖ Tombol 'Search' diklik.\n",
            "‚úÖ Berhasil mencapai halaman hasil pencarian penulis.\n",
            "‚úÖ Profil penulis diklik.\n",
            "‚è≥ Menggulir halaman profil penulis...\n",
            "‚úÖ Selesai menggulir halaman.\n",
            "Opsi display yang tersedia: ['10', '20', '50', '100', '200']\n",
            "‚úÖ Memilih '200' item per halaman.\n",
            "‚è≥ Mengambil data publikasi...\n",
            "Jumlah data ditemukan: 17\n",
            "‚úÖ Scraping Selesai. DataFrame dibuat.\n",
            "                   Article                                              Title  \\\n",
            "0                  Article  Modified tree-based selection in hierarchical ...   \n",
            "1  Article  ‚Ä¢  Open access  Improving the risk profile of Indonesian enter...   \n",
            "2                  Article  Rice phenology monitoring via ensemble classif...   \n",
            "3  Article  ‚Ä¢  Open access  Performance Comparative Study of Machine Learn...   \n",
            "4  Article  ‚Ä¢  Open access  MULTILEVEL REGRESSIONS FOR MODELING MEAN SCORE...   \n",
            "\n",
            "                                             Authors  \\\n",
            "0  Asrirawan, Notodiputro, K.A., Susetyo, B., Okt...   \n",
            "1              Prasetyo, T., Susetyo, B., Kurnia, A.   \n",
            "2  Kurniawati, Y., Wijayanto, H., Kurnia, A., Dir...   \n",
            "3  Khikmah, K.N., Sartono, B., Susetyo, B., Dito,...   \n",
            "4  Nurfadilah, K., Aidi, M.N., Notodiputro, K.A.,...   \n",
            "\n",
            "                                             Journal Year/Volume  \\\n",
            "0  Article, Methodsx\\n, , 14, 103312, Methodsx, 1...        2025   \n",
            "1  Article  ‚Ä¢  Open access, Open access, Iaes Int...        2024   \n",
            "2  Article, Remote Sensing Applications Society a...        2024   \n",
            "3  Article  ‚Ä¢  Open access, Open access, Jurnal O...        2024   \n",
            "4  Article  ‚Ä¢  Open access, Open access, Barekeng...        2024   \n",
            "\n",
            "                                         Article URL  \n",
            "0  https://www.scopus.com/pages/publications/1050...  \n",
            "1  https://www.scopus.com/pages/publications/8520...  \n",
            "2  https://www.scopus.com/pages/publications/8519...  \n",
            "3  https://www.scopus.com/pages/publications/1050...  \n",
            "4  https://www.scopus.com/pages/publications/8521...  \n",
            "‚úÖ Data disimpan ke 'scopus_data.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hasil data yang berhadil terambil dari Scopus ditampilkan di atas, lalu disimpan dalam file csv"
      ],
      "metadata": {
        "id": "1EZ-KKjfN525"
      },
      "id": "1EZ-KKjfN525"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Scrapping Data website Google Scholar\n",
        "\n",
        "\n",
        "Data yang akan diambil selanjutnya yaitu dari website Google Scholar dengan menggunakan API gratis. Karena website Google Scholar memiliki keamanan dengan menggunakan API, namun API yang digunakan di sini adalah API yang diambil dari SerpApi. Penggunaan API ini akan menggunakan library SerpApi yang sudah terintegrasi di dalam Python. Kemudian pengambilan data dilakukan secara berulang menggunakan fungsi ‚Äúwhile‚Äù agar semua artikel dapat terambil secara bertahap. Sama seperti sebelumnya elemen yang akan diambil adalah judul, penulis, publikasi, tahun, link dan jurnal"
      ],
      "metadata": {
        "id": "J_AasQ7c1wtv"
      },
      "id": "J_AasQ7c1wtv"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install google-search-results\n",
        "!pip install serpapi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uUkVYMaJ15W4",
        "outputId": "e1051c0a-2468-4316-b128-dc04517f2759"
      },
      "id": "uUkVYMaJ15W4",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting google-search-results\n",
            "  Downloading google_search_results-2.4.2.tar.gz (18 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from google-search-results) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->google-search-results) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->google-search-results) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->google-search-results) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->google-search-results) (2025.6.15)\n",
            "Building wheels for collected packages: google-search-results\n",
            "  Building wheel for google-search-results (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for google-search-results: filename=google_search_results-2.4.2-py3-none-any.whl size=32010 sha256=4339ba2e152ee04c8ac81c418c63531590c1a204b36080eb76b59ea9d594ea00\n",
            "  Stored in directory: /root/.cache/pip/wheels/6e/42/3e/aeb691b02cb7175ec70e2da04b5658d4739d2b41e5f73cd06f\n",
            "Successfully built google-search-results\n",
            "Installing collected packages: google-search-results\n",
            "Successfully installed google-search-results-2.4.2\n",
            "Collecting serpapi\n",
            "  Downloading serpapi-0.1.5-py2.py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from serpapi) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->serpapi) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->serpapi) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->serpapi) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->serpapi) (2025.6.15)\n",
            "Downloading serpapi-0.1.5-py2.py3-none-any.whl (10 kB)\n",
            "Installing collected packages: serpapi\n",
            "Successfully installed serpapi-0.1.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from serpapi.google_search import GoogleSearch\n",
        "import pandas as pd\n",
        "import time"
      ],
      "metadata": {
        "id": "E_NyO2D_16DK"
      },
      "id": "E_NyO2D_16DK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pertama, uninstall semua versi yang mungkin konflik\n",
        "!pip uninstall serpapi google-search-results -y\n",
        "\n",
        "# Install package yang benar\n",
        "!pip install google-search-results pandas\n",
        "\n",
        "# Tunggu instalasi selesai\n",
        "import time\n",
        "time.sleep(3)\n",
        "\n",
        "# Sekarang import dengan cara yang benar\n",
        "from serpapi.google_search import GoogleSearch\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "# API Key dan Author ID\n",
        "api_key = \"240ca5b83c3d0ccd7c7e278b2e430780135c424452e3a7a5ee7c441dbd296e53\"\n",
        "author_id = \"apmXydQAAAAJ\"\n",
        "\n",
        "all_articles = []\n",
        "start = 0\n",
        "max_retries = 3  # Jumlah maksimal percobaan ulang jika gagal\n",
        "\n",
        "while True:\n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            params = {\n",
        "                \"engine\": \"google_scholar_author\",\n",
        "                \"author_id\": author_id,\n",
        "                \"api_key\": api_key,\n",
        "                \"hl\": \"id\",\n",
        "                \"num\": 100,\n",
        "                \"start\": start\n",
        "            }\n",
        "\n",
        "            search = GoogleSearch(params)\n",
        "            results = search.get_dict()\n",
        "\n",
        "            articles = results.get(\"articles\", [])\n",
        "            if not articles:\n",
        "                print(\"‚úÖ Semua artikel berhasil diambil.\")\n",
        "                break\n",
        "\n",
        "            for article in articles:\n",
        "                # ‚úÖ Perbaikan di sini untuk \"Penulis\"\n",
        "                authors = article.get(\"authors\")\n",
        "                if isinstance(authors, list):\n",
        "                    penulis = \", \".join(authors)\n",
        "                else:\n",
        "                    penulis = authors if authors else \"N/A\"\n",
        "\n",
        "                all_articles.append({\n",
        "                    \"Judul\": article.get(\"title\"),\n",
        "                    \"Link\": article.get(\"link\"),\n",
        "                    \"Tahun\": article.get(\"year\"),\n",
        "                    \"Penulis\": penulis,\n",
        "                    \"Publikasi\": article.get(\"publication\", \"N/A\")\n",
        "                })\n",
        "\n",
        "            print(f\"üîÑ Halaman offset {start} - Artikel terkumpul: {len(all_articles)}\")\n",
        "            start += 100\n",
        "            time.sleep(2)  # delay kecil untuk jaga kuota\n",
        "            break\n",
        "\n",
        "        except Exception as e:\n",
        "            if attempt == max_retries - 1:\n",
        "                print(f\"‚ùå Gagal setelah {max_retries} percobaan: {str(e)}\")\n",
        "                break\n",
        "            print(f\"‚ö† Percobaan {attempt + 1} gagal, mencoba lagi...\")\n",
        "            time.sleep(5)\n",
        "    else:\n",
        "        break\n",
        "\n",
        "    if not articles:\n",
        "        break\n",
        "\n",
        "if all_articles:\n",
        "    df_scholar = pd.DataFrame(all_articles)\n",
        "    df_scholar = df_scholar.sort_values(by='Tahun', ascending=False)\n",
        "\n",
        "    filename = \"Google Scholar.csv\"\n",
        "    df_scholar.to_csv(filename, index=False, encoding='utf-8-sig')\n",
        "    print(f\"üìÅ Data disimpan ke '{filename}'\")\n",
        "\n",
        "    pd.set_option('display.max_columns', None)\n",
        "    pd.set_option('display.max_rows', None)\n",
        "    pd.set_option('display.max_colwidth', None)\n",
        "else:\n",
        "    print(\"‚ùå Tidak ada artikel yang berhasil diambil.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FkqUmWvg19l4",
        "outputId": "07272421-fbba-4550-8f2b-4a8e2adbdfca"
      },
      "id": "FkqUmWvg19l4",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: serpapi 0.1.5\n",
            "Uninstalling serpapi-0.1.5:\n",
            "  Successfully uninstalled serpapi-0.1.5\n",
            "Found existing installation: google_search_results 2.4.2\n",
            "Uninstalling google_search_results-2.4.2:\n",
            "  Successfully uninstalled google_search_results-2.4.2\n",
            "Collecting google-search-results\n",
            "  Using cached google_search_results-2.4.2-py3-none-any.whl\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from google-search-results) (2.32.3)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->google-search-results) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->google-search-results) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->google-search-results) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->google-search-results) (2025.6.15)\n",
            "Installing collected packages: google-search-results\n",
            "Successfully installed google-search-results-2.4.2\n",
            "üîÑ Halaman offset 0 - Artikel terkumpul: 100\n",
            "üîÑ Halaman offset 100 - Artikel terkumpul: 200\n",
            "üîÑ Halaman offset 200 - Artikel terkumpul: 293\n",
            "‚úÖ Semua artikel berhasil diambil.\n",
            "üìÅ Data disimpan ke 'Google Scholar.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nüìä 5 DATA TERATAS:\")\n",
        "print(df_scholar.head(5))  # Angka 5 bisa diubah sesuai kebutuhan"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mw8nCq572_xI",
        "outputId": "a8552ccc-1775-4f2a-900b-0f2ca268b141"
      },
      "id": "mw8nCq572_xI",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä 5 DATA TERATAS:\n",
            "                                                                                                                                                                                  Judul  \\\n",
            "157                                                                                    Simulation and Empirical Studies of Long Short-Term Memory Performance to Deal with Limited Data   \n",
            "164                                                                                                         THE COMPARISON OF ARIMA AND RNN FOR FORECASTING GOLD FUTURES CLOSING PRICES   \n",
            "158  Fay-Herriot Model with Random Forest-Based Fixed Effect Estimation for Small Area Estimation of Per Capita Expenditure among Poor and Agricultural Workers: Will be presented at ‚Ä¶   \n",
            "168                                                                                          Evaluation of machine learning models based on household food insecurity data in Indonesia   \n",
            "167                                                                         COMPARATIVE PERFORMANCE OF SARIMAX AND LSTM MODEL IN PREDICTING IMPORT QUANTITIES OF MILK, BUTTER, AND EGGS   \n",
            "\n",
            "                                                                                                                                                       Link  \\\n",
            "157  https://scholar.google.com/citations?view_op=view_citation&hl=id&user=apmXydQAAAAJ&cstart=100&pagesize=100&citation_for_view=apmXydQAAAAJ:_OXeSy2IsFwC   \n",
            "164  https://scholar.google.com/citations?view_op=view_citation&hl=id&user=apmXydQAAAAJ&cstart=100&pagesize=100&citation_for_view=apmXydQAAAAJ:HeT0ZceujKMC   \n",
            "158  https://scholar.google.com/citations?view_op=view_citation&hl=id&user=apmXydQAAAAJ&cstart=100&pagesize=100&citation_for_view=apmXydQAAAAJ:GFxP56DSvIMC   \n",
            "168  https://scholar.google.com/citations?view_op=view_citation&hl=id&user=apmXydQAAAAJ&cstart=100&pagesize=100&citation_for_view=apmXydQAAAAJ:r_AWSJRzSzQC   \n",
            "167  https://scholar.google.com/citations?view_op=view_citation&hl=id&user=apmXydQAAAAJ&cstart=100&pagesize=100&citation_for_view=apmXydQAAAAJ:3htObqc8RwsC   \n",
            "\n",
            "    Tahun                                                             Penulis  \\\n",
            "157  2025                                 KN Khikmah, K Sadik, KA Notodiputro   \n",
            "164  2025      WA Pratiwi, AF Rizki, KA Notodiputro, Y Angraini, LNA Mualifah   \n",
            "158  2025                AS Bukhari, KA Notodiputro, I Indahwati, A Fitrianto   \n",
            "168  2025                    H Fransiska, AM Soleh, KA Notodiputro, E Erfiani   \n",
            "167  2025  GG Ghiffary, EDD Yanuari, KA Notodiputro, Y Angraini, LNA Mualifah   \n",
            "\n",
            "                                                                          Publikasi  \n",
            "157                                 Jurnal Online Informatika 10 (1), 216-226, 2025  \n",
            "164              BAREKENG: Jurnal Ilmu Matematika dan Terapan 19 (1), 397-406, 2025  \n",
            "158  The 2nd International Seminar on Tropical Bioresources Advancement and ‚Ä¶, 2025  \n",
            "168                                         BIO Web of Conferences 171, 02011, 2025  \n",
            "167              BAREKENG: Jurnal Ilmu Matematika dan Terapan 19 (1), 407-418, 2025  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Output di atas menampilkan 5 data teratas yang berhadil diambil melalui webscraper dari Google Scholar dan disimpan dalam CSV"
      ],
      "metadata": {
        "id": "IcXV3iM-NsZC"
      },
      "id": "IcXV3iM-NsZC"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Scrapping Data website SINTA\n",
        "\n",
        "Tujuannya juga sama seperti sebelumnya, melakukan scraping dnegan menggunakan library BeautifulSoup dengan membentuk fungsi untuk mengambil elemen-elemen dari web SINTA yaitu menggambil data profil penulis dan data artikelnya. Web sinta memiliki 3 jenis artikel, yaitu dari google scholar, Scopus dan Garuda"
      ],
      "metadata": {
        "id": "QQTn1lBM7AkH"
      },
      "id": "QQTn1lBM7AkH"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zv5iiQiagJcA",
        "outputId": "bc2a4afc-4a12-41b2-a9f0-83f00767e59a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (4.13.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.6.15)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (2.7)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (4.13.2)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "pip install requests beautifulsoup4 pandas"
      ],
      "id": "Zv5iiQiagJcA"
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Gunakan fungsi parse_profil_sinta dari definisi\n",
        "def parse_profil_sinta(html_soup):\n",
        "    profil = {}\n",
        "    try:\n",
        "        # Nama\n",
        "        nama_elem = html_soup.select_one(\"div.col-lg.col-md > h3 a\")\n",
        "        profil[\"Nama Lengkap\"] = nama_elem.text.strip() if nama_elem else \"N/A\"\n",
        "\n",
        "        # Afiliasi, Program Studi, SINTA ID\n",
        "        meta_links = html_soup.select(\".meta-profile a\")\n",
        "        profil[\"Afiliasi\"] = meta_links[0].text.strip() if len(meta_links) > 0 else \"N/A\"\n",
        "        profil[\"Program Studi\"] = meta_links[1].text.strip() if len(meta_links) > 1 else \"N/A\"\n",
        "        profil[\"SINTA ID\"] = meta_links[2].text.strip().split(\":\")[-1].strip() if len(meta_links) > 2 else \"N/A\"\n",
        "\n",
        "        # Skor-skor\n",
        "        skor_elements = html_soup.select(\".stat-profile .pr-num\")\n",
        "        profil[\"SINTA Score Overall\"] = skor_elements[0].text.strip() if len(skor_elements) > 0 else \"N/A\"\n",
        "        profil[\"SINTA Score 3Yr\"] = skor_elements[1].text.strip() if len(skor_elements) > 1 else \"N/A\"\n",
        "        profil[\"Affil Score\"] = skor_elements[2].text.strip() if len(skor_elements) > 2 else \"N/A\"\n",
        "        profil[\"Affil Score 3Yr\"] = skor_elements[3].text.strip() if len(skor_elements) > 3 else \"N/A\"\n",
        "\n",
        "        # Tabel Statistik (Scopus, GScholar)\n",
        "        metric_table = html_soup.select_one(\".stat-table tbody\")\n",
        "        if metric_table:\n",
        "            for row in metric_table.find_all(\"tr\"):\n",
        "                cells = row.find_all(\"td\")\n",
        "                if len(cells) >= 3:\n",
        "                    key = cells[0].text.strip()\n",
        "                    profil[f\"{key} (Scopus)\"] = cells[1].text.strip()\n",
        "                    profil[f\"{key} (GScholar)\"] = cells[2].text.strip()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö† Error parsing profil: {e}\")\n",
        "    return profil\n",
        "\n",
        "# Ambil HTML profil penulis\n",
        "author_id = \"6173018\"\n",
        "url = f\"https://sinta.kemdikbud.go.id/authors/profile/{author_id}\"\n",
        "\n",
        "headers = {\n",
        "    \"User-Agent\": \"Mozilla/5.0\",\n",
        "    \"Accept-Language\": \"id-ID,id;q=0.9,en-US;q=0.8,en;q=0.7\"\n",
        "}\n",
        "\n",
        "response = requests.get(url, headers=headers)\n",
        "soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "# Panggil fungsi profil\n",
        "profil_data = parse_profil_sinta(soup)\n",
        "\n",
        "# Tampilkan hasilnya\n",
        "print(\"\\n=== PROFIL PENULIS SINTA ===\")\n",
        "for k, v in profil_data.items():\n",
        "    print(f\"{k}: {v}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gfizO1jalPse",
        "outputId": "afbb9864-e5af-4060-f52e-a80cab6311d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== PROFIL PENULIS SINTA ===\n",
            "Nama Lengkap: BUDI SUSETYO\n",
            "Afiliasi: Institut Pertanian Bogor\n",
            "Program Studi: S2 - Statistika dan Sains Data\n",
            "SINTA ID: 6173018\n",
            "SINTA Score Overall: 793\n",
            "SINTA Score 3Yr: 257\n",
            "Affil Score: 793\n",
            "Affil Score 3Yr: 257\n",
            "Article (Scopus): 17\n",
            "Article (GScholar): 100\n",
            "Citation (Scopus): 16\n",
            "Citation (GScholar): 335\n",
            "Cited Document (Scopus): 11\n",
            "Cited Document (GScholar): 61\n",
            "H-Index (Scopus): 2\n",
            "H-Index (GScholar): 9\n",
            "i10-Index (Scopus): 0\n",
            "i10-Index (GScholar): 9\n",
            "G-Index (Scopus): 1\n",
            "G-Index (GScholar): 1\n"
          ]
        }
      ],
      "id": "gfizO1jalPse"
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import re\n",
        "import time # Import modul time untuk jeda\n",
        "\n",
        "def parse_profil_sinta(html_soup):\n",
        "    profil = {}\n",
        "    try:\n",
        "        profil[\"Nama Lengkap\"] = html_soup.select_one(\"div.col-lg.col-md > h3 a\").text.strip()\n",
        "        profil[\"Afiliasi\"] = html_soup.select_one(\".meta-profile a:nth-child(1)\").text.strip()\n",
        "        profil[\"Program Studi\"] = html_soup.select_one(\".meta-profile a:nth-child(3)\").text.strip()\n",
        "        profil[\"SINTA ID\"] = html_soup.select_one(\".meta-profile a:nth-child(5)\").text.split(\":\")[-1].strip()\n",
        "\n",
        "        skor_elements = html_soup.select(\".stat-profile .pr-num\")\n",
        "        profil[\"SINTA Score Overall\"] = skor_elements[0].text.strip() if len(skor_elements) > 0 else \"N/A\"\n",
        "        profil[\"SINTA Score 3Yr\"] = skor_elements[1].text.strip() if len(skor_elements) > 1 else \"N/A\"\n",
        "        profil[\"Affil Score\"] = skor_elements[2].text.strip() if len(skor_elements) > 2 else \"N/A\"\n",
        "        profil[\"Affil Score 3Yr\"] = skor_elements[3].text.strip() if len(skor_elements) > 3 else \"N/A\"\n",
        "\n",
        "        metric_table = html_soup.select_one(\".stat-table tbody\")\n",
        "        if metric_table:\n",
        "            for row in metric_table.find_all(\"tr\"):\n",
        "                cells = row.find_all(\"td\")\n",
        "                if len(cells) >= 3:\n",
        "                    key = cells[0].text.strip()\n",
        "                    profil[f\"{key} (Scopus)\"] = cells[1].text.strip()\n",
        "                    profil[f\"{key} (GScholar)\"] = cells[2].text.strip()\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö† Error parsing profil: {e}\")\n",
        "    return profil\n",
        "\n",
        "def scrape_sinta_selected_views(author_id, timeout=45, max_retries=5, delay_between_views=3): # Timeout dinaikkan, jeda antar view\n",
        "    base_url = f\"https://sinta.kemdikbud.go.id/authors/profile/{author_id}\"\n",
        "    views = ['googlescholar', 'scopus', 'garuda']\n",
        "    all_data = []\n",
        "    profil_data = {}\n",
        "\n",
        "    headers = {\n",
        "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\",\n",
        "        \"Accept-Language\": \"id-ID,id;q=0.9,en-US;q=0.8,en;q=0.7\"\n",
        "    }\n",
        "\n",
        "    # --- Bagian untuk mengambil profil utama dengan retry ---\n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            print(f\"Percobaan {attempt + 1}/{max_retries} untuk mengambil halaman profil utama...\")\n",
        "            profil_response = requests.get(base_url, headers=headers, timeout=timeout)\n",
        "            profil_response.raise_for_status()\n",
        "            soup_profil = BeautifulSoup(profil_response.text, 'html.parser')\n",
        "            profil_data = parse_profil_sinta(soup_profil)\n",
        "            print(\"‚úÖ Profil utama berhasil diambil.\")\n",
        "            break\n",
        "        except requests.exceptions.Timeout:\n",
        "            print(f\"‚ö†Ô∏è Timeout saat mengambil halaman profil utama (Percobaan {attempt + 1}/{max_retries}). Mencoba lagi...\")\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            print(f\"‚ö† Gagal mengambil halaman profil utama (Percobaan {attempt + 1}/{max_retries}): {e}\")\n",
        "\n",
        "        if attempt < max_retries - 1:\n",
        "            time.sleep(2 ** attempt)\n",
        "        else:\n",
        "            print(\"‚ùó Maksimal percobaan tercapai untuk halaman profil. Tidak dapat mengambil profil.\")\n",
        "    # --- Akhir bagian profil utama ---\n",
        "\n",
        "    # --- Bagian untuk mengambil data dari masing-masing view publikasi dengan retry ---\n",
        "    for view in views:\n",
        "        url = f\"{base_url}/?view={view}\"\n",
        "        print(f\"\\nüì• Mengambil data dari: {view.upper()} - {url}\")\n",
        "\n",
        "        # Tambahkan jeda singkat sebelum memulai scraping untuk view baru\n",
        "        if delay_between_views > 0:\n",
        "            print(f\"Menunggu {delay_between_views} detik sebelum mengambil view {view.upper()}...\")\n",
        "            time.sleep(delay_between_views)\n",
        "\n",
        "        for attempt in range(max_retries):\n",
        "            try:\n",
        "                print(f\"Percobaan {attempt + 1}/{max_retries} untuk mengambil data dari {view.upper()}...\")\n",
        "                response = requests.get(url, headers=headers, timeout=timeout)\n",
        "                response.raise_for_status()\n",
        "                soup = BeautifulSoup(response.text, 'html.parser')\n",
        "                print(f\"‚úÖ Data {view.upper()} berhasil diambil.\")\n",
        "\n",
        "                articles = soup.select('div.ar-list-item')\n",
        "                if not articles:\n",
        "                    print(f\"Tidak ada artikel ditemukan untuk view '{view}'.\")\n",
        "                    break\n",
        "\n",
        "                for article in articles:\n",
        "                    try:\n",
        "                        title_elem = article.select_one('div.ar-title a')\n",
        "                        title = title_elem.get_text(strip=True) if title_elem else \"N/A\"\n",
        "                        link = title_elem['href'] if title_elem else \"N/A\"\n",
        "\n",
        "                        authors_elem = article.select_one('div.ar-meta a')\n",
        "                        authors = [\"N/A\"]\n",
        "                        if authors_elem and 'Authors :' in authors_elem.get_text():\n",
        "                            authors_text = authors_elem.get_text(strip=True).replace('Authors :', '').strip()\n",
        "                            authors = [a.strip() for a in authors_text.split(',')]\n",
        "\n",
        "                        journal_elem = article.select_one('a.ar-pub')\n",
        "                        journal = journal_elem.get_text(strip=True).replace('\\xa0', ' ').strip() if journal_elem else \"N/A\"\n",
        "                        journal = re.sub(r',\\s*(\\d{4})$', '', journal)\n",
        "\n",
        "                        # Perbaikan untuk masalah 'NoneType' object has no attribute 'group'\n",
        "                        year = \"N/A\"\n",
        "                        if article.select_one('a.ar-year'): # Coba ambil dari elemen tahun yang spesifik dulu\n",
        "                            year_match = re.search(r'\\d{4}', article.select_one('a.ar-year').get_text(strip=True))\n",
        "                            if year_match:\n",
        "                                year = year_match.group(0)\n",
        "\n",
        "                        # Fallback jika tahun tidak ditemukan di a.ar-year atau jika year_elem tidak ada\n",
        "                        if year == \"N/A\" and journal_elem:\n",
        "                            year_match_journal = re.search(r'\\d{4}', journal_elem.get_text(strip=True))\n",
        "                            if year_match_journal:\n",
        "                                year = year_match_journal.group(0)\n",
        "\n",
        "                        cited_elem = article.select_one('a.ar-cited')\n",
        "                        cited = re.search(r'\\d+', cited_elem.get_text(strip=True)).group(0) if cited_elem and re.search(r'\\d+', cited_elem.get_text(strip=True)) else \"0\"\n",
        "\n",
        "\n",
        "                        all_data.append({\n",
        "                            'Sumber': view.upper(),\n",
        "                            'Judul': title,\n",
        "                            'Penulis': '; '.join(authors),\n",
        "                            'Jurnal': journal,\n",
        "                            'Tahun': year,\n",
        "                            'Cited': cited,\n",
        "                            'Link': link\n",
        "                        })\n",
        "                    except Exception as e:\n",
        "                        print(f\"‚ö† Error saat memproses artikel pada view '{view}': {str(e)}\")\n",
        "                        continue\n",
        "                break\n",
        "\n",
        "            except requests.exceptions.Timeout:\n",
        "                print(f\"‚ö†Ô∏è Timeout saat mengambil data '{view}' (Percobaan {attempt + 1}/{max_retries}). Mencoba lagi...\")\n",
        "            except requests.exceptions.RequestException as e:\n",
        "                print(f\"Request error untuk view '{view}' (Percobaan {attempt + 1}/{max_retries}): {str(e)}\")\n",
        "\n",
        "            if attempt < max_retries - 1:\n",
        "                time.sleep(2 ** attempt)\n",
        "            else:\n",
        "                print(f\"‚ùó Maksimal percobaan tercapai untuk view '{view}'. Tidak dapat mengambil data.\")\n",
        "    # --- Akhir bagian view publikasi ---\n",
        "\n",
        "    return profil_data, pd.DataFrame(all_data)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    author_id = \"6173018\"\n",
        "    print(f\"Memulai scraping untuk SINTA ID: {author_id}\")\n",
        "    profil, df_artikel = scrape_sinta_selected_views(author_id)\n",
        "\n",
        "    print(\"\\n---\")\n",
        "    print(\"üìÑ Profil Penulis:\")\n",
        "    if profil:\n",
        "        for key, value in profil.items():\n",
        "            print(f\"{key}: {value}\")\n",
        "    else:\n",
        "        print(\"Profil penulis tidak berhasil diambil.\")\n",
        "\n",
        "    print(\"\\n---\")\n",
        "    if df_artikel.empty:\n",
        "        print(\"Tidak ada data artikel yang berhasil diambil.\")\n",
        "    else:\n",
        "        print(f\"üìë Total Artikel Berhasil Diambil: {len(df_artikel)}\")\n",
        "        print(\"\\n5 Artikel Pertama:\")\n",
        "        print(df_artikel.head())\n",
        "\n",
        "        try:\n",
        "            df_artikel.to_csv(f\"sinta_articles_{author_id}.csv\", index=False, encoding='utf-8')\n",
        "            # Pastikan profil_data tidak kosong sebelum menyimpan\n",
        "            if profil:\n",
        "                pd.DataFrame([profil]).to_csv(f\"sinta_profile_{author_id}.csv\", index=False, encoding='utf-8')\n",
        "            else:\n",
        "                print(\"Tidak dapat menyimpan profil karena kosong.\")\n",
        "            print(f\"\\n‚úÖ Data profil dan artikel disimpan ke file CSV.\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Gagal menyimpan file CSV: {e}\")\n",
        "\n",
        "    print(\"\\n--- Selesai ---\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dB0ASzMU2i4q",
        "outputId": "81c8f0c3-b19e-45ac-9a8c-57fafe016c0d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Memulai scraping untuk SINTA ID: 6173018\n",
            "Percobaan 1/5 untuk mengambil halaman profil utama...\n",
            "‚úÖ Profil utama berhasil diambil.\n",
            "\n",
            "üì• Mengambil data dari: GOOGLESCHOLAR - https://sinta.kemdikbud.go.id/authors/profile/6173018/?view=googlescholar\n",
            "Menunggu 3 detik sebelum mengambil view GOOGLESCHOLAR...\n",
            "Percobaan 1/5 untuk mengambil data dari GOOGLESCHOLAR...\n",
            "‚úÖ Data GOOGLESCHOLAR berhasil diambil.\n",
            "\n",
            "üì• Mengambil data dari: SCOPUS - https://sinta.kemdikbud.go.id/authors/profile/6173018/?view=scopus\n",
            "Menunggu 3 detik sebelum mengambil view SCOPUS...\n",
            "Percobaan 1/5 untuk mengambil data dari SCOPUS...\n",
            "‚úÖ Data SCOPUS berhasil diambil.\n",
            "\n",
            "üì• Mengambil data dari: GARUDA - https://sinta.kemdikbud.go.id/authors/profile/6173018/?view=garuda\n",
            "Menunggu 3 detik sebelum mengambil view GARUDA...\n",
            "Percobaan 1/5 untuk mengambil data dari GARUDA...\n",
            "‚úÖ Data GARUDA berhasil diambil.\n",
            "\n",
            "---\n",
            "üìÑ Profil Penulis:\n",
            "Nama Lengkap: BUDI SUSETYO\n",
            "Afiliasi: Institut Pertanian Bogor\n",
            "Program Studi: S2 - Statistika dan Sains Data\n",
            "SINTA ID: 6173018\n",
            "SINTA Score Overall: 793\n",
            "SINTA Score 3Yr: 257\n",
            "Affil Score: 793\n",
            "Affil Score 3Yr: 257\n",
            "Article (Scopus): 17\n",
            "Article (GScholar): 100\n",
            "Citation (Scopus): 16\n",
            "Citation (GScholar): 335\n",
            "Cited Document (Scopus): 11\n",
            "Cited Document (GScholar): 61\n",
            "H-Index (Scopus): 2\n",
            "H-Index (GScholar): 9\n",
            "i10-Index (Scopus): 0\n",
            "i10-Index (GScholar): 9\n",
            "G-Index (Scopus): 1\n",
            "G-Index (GScholar): 1\n",
            "\n",
            "---\n",
            "üìë Total Artikel Berhasil Diambil: 30\n",
            "\n",
            "5 Artikel Pertama:\n",
            "          Sumber  \\\n",
            "0  GOOGLESCHOLAR   \n",
            "1  GOOGLESCHOLAR   \n",
            "2  GOOGLESCHOLAR   \n",
            "3  GOOGLESCHOLAR   \n",
            "4  GOOGLESCHOLAR   \n",
            "\n",
            "                                                                                                                                        Judul  \\\n",
            "0                     PENERAPAN ANALISIS REGRESI LOGISTIK ORDINAL MULTILEVEL DENGAN BAYESIAN DALAM MEMODELKAN TINGKAT KESEJAHTERAAN DATA P3KE   \n",
            "1                  Modified tree-based selection in hierarchical mixed-effect models with trees: A simulation study and real-data application   \n",
            "2  Exploring the Potential Impact of Ginger Consumption on the Duration of COVID-19 Recovery: A Propensity Score Matching using Random Forest   \n",
            "3                     PENERAPAN ANALISIS REGRESI LOGISTIK ORDINAL MULTILEVEL DENGAN BAYESIAN DALAM MEMODELKAN TINGKAT KESEJAHTERAAN DATA P3KE   \n",
            "4               Rice phenology monitoring via ensemble classification for an extremely imbalanced multiclass dataset of hybrid remote sensing   \n",
            "\n",
            "                                          Penulis  \\\n",
            "0                 N Hermawati; B Susetyo; K Sadik   \n",
            "1          KA Notodiputro; B Susetyo; SD Oktarina   \n",
            "2                 RDS Nufus; B Susetyo; B Sartono   \n",
            "3                 N Hermawati; B Susetyo; K Sadik   \n",
            "4  Y Kurniawati; H Wijayanto; A Kurnia; B Susetyo   \n",
            "\n",
            "                                                                    Jurnal  \\\n",
            "0   Jurnal Lebesgue: Jurnal Ilmiah Pendidikan Matematika, Matematika dan ‚Ä¶   \n",
            "1                                                      MethodsX 14, 103312   \n",
            "2  IOP Conference Series: Earth and Environmental Science 1359 (1), 012139   \n",
            "3   Jurnal Lebesgue: Jurnal Ilmiah Pendidikan Matematika, Matematika dan ‚Ä¶   \n",
            "4          Remote Sensing Applications: Society and Environment 35, 101246   \n",
            "\n",
            "  Tahun Cited  \\\n",
            "0  2025     0   \n",
            "1  2025     0   \n",
            "2  2024     0   \n",
            "3  2024     0   \n",
            "4  2024     1   \n",
            "\n",
            "                                                                                                                                                                                         Link  \n",
            "0                     https://scholar.google.com/scholar?q=+intitle:'PENERAPAN ANALISIS REGRESI LOGISTIK ORDINAL MULTILEVEL DENGAN BAYESIAN DALAM MEMODELKAN TINGKAT KESEJAHTERAAN DATA P3KE'  \n",
            "1                  https://scholar.google.com/scholar?q=+intitle:'Modified tree-based selection in hierarchical mixed-effect models with trees: A simulation study and real-data application'  \n",
            "2  https://scholar.google.com/scholar?q=+intitle:'Exploring the Potential Impact of Ginger Consumption on the Duration of COVID-19 Recovery: A Propensity Score Matching using Random Forest'  \n",
            "3                     https://scholar.google.com/scholar?q=+intitle:'PENERAPAN ANALISIS REGRESI LOGISTIK ORDINAL MULTILEVEL DENGAN BAYESIAN DALAM MEMODELKAN TINGKAT KESEJAHTERAAN DATA P3KE'  \n",
            "4               https://scholar.google.com/scholar?q=+intitle:'Rice phenology monitoring via ensemble classification for an extremely imbalanced multiclass dataset of hybrid remote sensing'  \n",
            "\n",
            "‚úÖ Data profil dan artikel disimpan ke file CSV.\n",
            "\n",
            "--- Selesai ---\n"
          ]
        }
      ],
      "id": "dB0ASzMU2i4q"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Maka data berhasil diambil dan disimpan dalam bentuk csv"
      ],
      "metadata": {
        "id": "P-z3bMQ7Nk1Q"
      },
      "id": "P-z3bMQ7Nk1Q"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.1"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}